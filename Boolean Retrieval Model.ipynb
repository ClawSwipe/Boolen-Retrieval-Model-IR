{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiMoMofOKsK_"
      },
      "outputs": [],
      "source": [
        "#made on Google Colab, best results if checked there. All ResearchPapers and Stopword-List uploaded to google colab directory (click on the folder icon on the left)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('Stopword-List.txt','r')\n",
        "c = f.read()\n",
        "f.close()\n",
        "stopwords = c.split('\\n')\n"
      ],
      "metadata": {
        "id": "f3h06LecRg3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(content):\n",
        "    content = content.lower()\n",
        "    content = content.replace(\"-\", \" \")\n",
        "    content = content.replace(\"â€¢\",\" \")\n",
        "    content = content.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    words = re.split(r'\\s+|\\n+', content)\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "HQ83uTfcys2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess a document\n",
        "def preprocess_document(doc):\n",
        "    # Tokenize the document\n",
        "    words = tokenize(doc)\n",
        "    # Remove stopwords and apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    filtered_words = [stemmer.stem(word) for word in words if word.lower() not in stopwords]\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "# Function to clean text and generate positions\n",
        "def clean_and_generate_positions(docs_directory):\n",
        "    # Build inverted index\n",
        "    inverted_index = {}\n",
        "\n",
        "    # Loop through each document\n",
        "    for filename in os.listdir(docs_directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Read the content of the document\n",
        "            with open(os.path.join(docs_directory, filename), 'r', encoding='ISO-8859-1') as file:\n",
        "                document_content = file.read()\n",
        "                document_content = re.sub(r'https?://(?:www\\.)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', document_content)  # remove urls\n",
        "                document_content = re.sub(r'\\S+\\.com\\b', '', document_content)  # remove .com\n",
        "                document_content = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', document_content)  # remove emails\n",
        "                document_content = re.sub(r'\\b\\d+\\b', '', document_content)  # remove numbers\n",
        "                document_content = re.sub(r'[^\\w\\s]', '', document_content)  # remove other useless punctuation\n",
        "            # Preprocess the document\n",
        "            processed_words = preprocess_document(document_content)\n",
        "            # Update inverted index\n",
        "            for position, word in enumerate(processed_words):\n",
        "                if word not in inverted_index:\n",
        "                    inverted_index[word] = []\n",
        "                inverted_index[word].append((filename, position))\n",
        "\n",
        "    return inverted_index\n",
        "\n",
        "# Example usage\n",
        "docs_directory = \"/content/\"\n",
        "inverted_index = clean_and_generate_positions(docs_directory)\n",
        "# print(inverted_index)\n",
        "\n",
        "count = 0\n",
        "for term in inverted_index:\n",
        "  count+=1\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an1JJSM4-tlP",
        "outputId": "3ad622e6-e190-4ec0-c34b-266ccc64ec0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def boolean_or(query_terms, inverted_index):\n",
        "    result = set()\n",
        "\n",
        "    for term in query_terms:\n",
        "        stemmed_term = stemmer.stem(term)\n",
        "        postings = set(inverted_index.get(stemmed_term, []))\n",
        "        result.update(entry[0] for entry in postings)\n",
        "\n",
        "    result = list(result)\n",
        "\n",
        "    sorted_result = sorted(list(result), key=lambda x: int(x.split('.')[0]) if '.' in x else int(x))\n",
        "\n",
        "    return sorted_result\n",
        "\n",
        "# Example OR query\n",
        "or_query = ['transformer', 'model']\n",
        "result_or = boolean_or(or_query, inverted_index)\n",
        "\n",
        "result_or = [doc.replace('.txt','') for doc in result_or]\n",
        "# Print the result without \"txt\" extension\n",
        "print(f\"OR Query Result: {result_or}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em8CqLEJwrXH",
        "outputId": "c51d61dc-7a7f-4feb-bc3c-b23e6a57f991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Query Result: ['1', '2', '3', '7', '8', '9', '12', '13', '14', '15', '16', '17', '18', '21', '22', '23', '24', '25', '26']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getposting(term,inverted_index):\n",
        "  result = set()\n",
        "  term = stemmer.stem(term)\n",
        "  postings = set(inverted_index.get(term,[]))\n",
        "  result.update(entry[0] for entry in postings)\n",
        "\n",
        "  sorted_result = sorted(list(result), key=lambda x: int(x.split('.')[0]) if '.' in x else int(x))\n",
        "  return sorted_result\n",
        "\n"
      ],
      "metadata": {
        "id": "uYeMm4lQkvLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def boolean_and(query_terms, inverted_index):\n",
        "    result = None\n",
        "    common_docs = set()\n",
        "    for term in query_terms:\n",
        "        stemmed_term = stemmer.stem(term)\n",
        "        postings = set(inverted_index.get(stemmed_term, []))\n",
        "        if result is None:\n",
        "            result = postings\n",
        "        else:\n",
        "            # Take the intersection of postings based on document name only\n",
        "            common_docs = {entry[0] for entry in result}.intersection(entry[0] for entry in postings)\n",
        "    # Convert the result to a sorted list by docID\n",
        "    sorted_result = sorted(list(common_docs), key=lambda x: int(x.split('.')[0]) if '.' in x else int(x))\n",
        "\n",
        "    return sorted_result\n",
        "\n",
        "# Example AND query\n",
        "and_query = ['feature', 'selection','redundancy']\n",
        "result_and = boolean_and(and_query, inverted_index)\n",
        "\n",
        "result_and = [doc.replace('.txt','') for doc in result_and]\n",
        "# Print the sorted result\n",
        "print(f\" AND Query Result: {result_and}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJnPRXsdc5tW",
        "outputId": "27d4b045-1bb8-486c-fcd9-8dbe56cd6834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AND Query Result: ['22', '23', '24', '25', '26']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "docs_directory = \"/content/\"\n",
        "\n",
        "def boolean_not(not_query_terms, inverted_index):\n",
        "    # Find documents containing the terms from the NOT query\n",
        "    not_docs = set()\n",
        "    result = set()\n",
        "\n",
        "    for term in not_query_terms:\n",
        "        stemmed_term = stemmer.stem(term)\n",
        "        postings = set(inverted_index.get(stemmed_term, []))\n",
        "        not_docs.update(entry[0] for entry in postings)\n",
        "\n",
        "\n",
        "\n",
        "    for filename in os.listdir(docs_directory):\n",
        "        if filename not in not_docs and filename.endswith(\".txt\") and filename != \"Stopword-List.txt\":\n",
        "            result.add(filename)\n",
        "\n",
        "\n",
        "    # Sort the result by docID\n",
        "    sorted_result = sorted(result, key=lambda x: int(x.split('.')[0]) if '.' in x else int(x))\n",
        "    return sorted_result\n",
        "\n",
        "# Example NOT query\n",
        "not_query = ['model']\n",
        "\n",
        "result_not = boolean_not(not_query, inverted_index)\n",
        "\n",
        "result_not = [doc.replace('.txt', '') for doc in result_not]\n",
        "# Print the result without \"txt\" extension\n",
        "print(f\"NOT Query Result: {result_not}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBbiPtKC9ITr",
        "outputId": "63ee68fa-a23c-4232-d0ec-d44edca6887d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT Query Result: ['11']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def convertexpressiontopostfix(expression):\n",
        "    # Using a stack to convert the user-entered query to a postfix expression\n",
        "    precedence = {'(': 0, 'NOT': 1, 'AND': 2, 'OR': 3}\n",
        "    stack = []\n",
        "    output = []\n",
        "\n",
        "    # Split the expression into words\n",
        "    words = expression.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in ['NOT', 'AND', 'OR']:\n",
        "            while stack and stack[-1] != '(' and precedence[word] < precedence[stack[-1]]:\n",
        "                output.append(stack.pop())\n",
        "            stack.append(word)\n",
        "        elif word == '(':\n",
        "            stack.append(word)\n",
        "        elif word == ')':\n",
        "            while stack and stack[-1] != '(':\n",
        "                output.append(stack.pop())\n",
        "            if stack and stack[-1] == '(':\n",
        "                stack.pop()\n",
        "        else:\n",
        "            output.append(word)\n",
        "\n",
        "    while stack:\n",
        "        output.append(stack.pop())\n",
        "\n",
        "    # Join the elements with space to form a valid postfix expression\n",
        "    return ' '.join(output)\n",
        "\n",
        "def evaluatepostfixexpression(expression):\n",
        "    print('postfix:', expression)\n",
        "    # Evaluating the postfix expression using operators precedences\n",
        "    stack = []\n",
        "    operators = {'AND', 'OR', 'NOT'}\n",
        "    precedence = {'NOT': 1, 'AND': 2, 'OR': 3}\n",
        "\n",
        "    for word in expression.split(' '):\n",
        "        if word not in operators:\n",
        "            stack.append(word)\n",
        "        else:\n",
        "            if word == 'NOT':\n",
        "                element = stack.pop()\n",
        "                result = boolean_not([element], inverted_index)\n",
        "            else:\n",
        "                while stack and stack[-1] in operators and precedence[word] <= precedence[stack[-1]]:\n",
        "                    operator = stack.pop()\n",
        "                    if operator == 'NOT':\n",
        "                        operand = stack.pop()\n",
        "                        result = boolean_not([operand], inverted_index)\n",
        "                    else:\n",
        "                        operand2 = stack.pop()\n",
        "                        operand1 = stack.pop()\n",
        "                        if operator == 'AND':\n",
        "                            result = boolean_and([operand1, operand2], inverted_index)\n",
        "                        else:\n",
        "                            result = boolean_or([operand1, operand2], inverted_index)\n",
        "                    stack.append(result)\n",
        "                stack.append(word)\n",
        "\n",
        "    while stack:\n",
        "        operator = stack.pop()\n",
        "        if operator == 'NOT':\n",
        "            operand = stack.pop()\n",
        "            result = boolean_not([operand], inverted_index)\n",
        "        else:\n",
        "            operand2 = stack.pop()\n",
        "            operand1 = stack.pop()\n",
        "            if operator == 'AND':\n",
        "                result = boolean_and([operand1, operand2], inverted_index)\n",
        "            else:\n",
        "                result = boolean_or([operand1, operand2], inverted_index)\n",
        "        stack.append(result)\n",
        "\n",
        "    return stack.pop()\n",
        "\n",
        "def has_only_and(expression):\n",
        "    operators = [operator for operator in expression.split() if operator in {'AND', 'OR', 'NOT'}]\n",
        "    return all(operator == 'AND' for operator in operators)\n",
        "\n",
        "\n",
        "def has_only_or(expression):\n",
        "    operators = [operator for operator in expression.split() if operator in {'AND', 'OR', 'NOT'}]\n",
        "    return all(operator == 'OR' for operator in operators)\n",
        "\n",
        "\n",
        "def has_only_one_word(user_input):\n",
        "    words = user_input.split()\n",
        "    return len(words) == 1\n",
        "\n",
        "def proximity_query(word1, word2, k, inverted_index):\n",
        "    stemmed_word1 = stemmer.stem(word1)\n",
        "    stemmed_word2 = stemmer.stem(word2)\n",
        "\n",
        "    postings_word1 = inverted_index.get(stemmed_word1, [])\n",
        "    postings_word2 = inverted_index.get(stemmed_word2, [])\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for doc1, pos1 in postings_word1:\n",
        "        for doc2, pos2 in postings_word2:\n",
        "            if doc1 == doc2:\n",
        "                # Check if there are any positions within the specified distance k\n",
        "                if (abs(pos1 - pos2) == k):\n",
        "                    result.append(doc1)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def is_proximity_query(user_input):\n",
        "    # Check if the user input matches the format for a proximity query\n",
        "    # Assuming the format is \"word1 word2 /k\" where k is an integer\n",
        "    tokens = user_input.split()\n",
        "    return len(tokens) == 3 and tokens[2].startswith('/')\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  # Get user input for the query\n",
        "  user_input = input(\"Enter your query: \")\n",
        "\n",
        "  if is_proximity_query(user_input):\n",
        "    # Extract information from the proximity query\n",
        "    words, k = user_input.split()[:2], int(user_input.split('/')[1])\n",
        "    # Call the proximity query function\n",
        "    result = proximity_query(words[0], words[1], k, inverted_index)\n",
        "    print(\"Documents where '{}' and '{}' are {} distance apart: {}\".format(words[0], words[1], k, result))\n",
        "\n",
        "\n",
        "  elif has_only_one_word(user_input):\n",
        "    result = getposting(user_input,inverted_index)\n",
        "    print(result)\n",
        "  elif has_only_and(user_input):\n",
        "    # Extract words and call boolean_and\n",
        "    query_terms = user_input.split()\n",
        "    query_terms = [term for term in query_terms if term != 'AND']\n",
        "    print(query_terms)\n",
        "    result = boolean_and(query_terms, inverted_index)\n",
        "    print(\"Query Result:\", result)\n",
        "\n",
        "  elif has_only_or(user_input):\n",
        "    # Extract words and call boolean_and\n",
        "    query_terms = user_input.split()\n",
        "    query_terms = [term for term in query_terms if term != 'OR']\n",
        "    print(query_terms)\n",
        "    result = boolean_or(query_terms, inverted_index)\n",
        "    print(\"Query Result:\", result)\n",
        "\n",
        "\n",
        "  else:\n",
        "    # Convert the infix expression to postfix\n",
        "    postfix_expression = convertexpressiontopostfix(user_input)\n",
        "\n",
        "    # Evaluate the postfix expression\n",
        "    result = evaluatepostfixexpression(postfix_expression)\n",
        "\n",
        "    # Print the result\n",
        "    print(\"Query Result:\", result)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "zRKNvKcsEGF2",
        "outputId": "d5602513-c3d6-4263-bdbd-ac817d5ebf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: heart AND metal\n",
            "['heart', 'metal']\n",
            "Query Result: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-335-7bf218d9826a>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;31m# Get user input for the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_proximity_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}